# A Survey of Transformers

收集有关transformer的各种技术与进展



# Efficient Transformers

Transformer中的self-attention机制，由于其O(n ^2)的时间和空间复杂度(n为序列长度)，因此很难拓展到非常长的序列上。为此后续不断有各种各样的**X-former**



# 1. FlashAttention

