# MuLi在交大的讲座记录



**在硬件方面**，分布式训练中，最重要的是带宽。

多卡最开始是分散在多台机器上，现在的趋势是尽量将多卡放在一台机器上，降低多机之间的通讯延迟。

散热也趋向于风冷转为水冷，从而能够在单台机器上塞下更多的GPU。



带宽之后就是内存。内存大小决定了模型的大小，内存在未来几年可能成为瓶颈。

近些年的HBM技术。



近几年NVIDIA通过低精度浮点数(FP4，FP8 E4M3，E5M2)来提升GPU的吞吐。

算力的提升伴随而来的是对于电力需求的提升。

目前在训练芯片方面并没有NVIDIA的替代品，推理芯片比如Intel和AMD有相应推理芯片。



**在模型方面**

目前的LLM：

- 预训练token 10-50T
- 100-500B参数量(MoE可能更多)
- Omni模型？



不同模态：

- Image与language 非常成熟
- Video 还不是很成熟
- MultiModal是目前的趋势，语言信息含量高并且容易获取，利用模型从语言方面的能力泛化到其它模块。并且文本能够训练定制化模型，能够控制模型行为。



语言模型的发展同时也带来了人机交互方面的改变。

但是目前还没有一个对应的killer app。



**在应用方面**

AI在处理blue collar的任务方面非常弱。因为blue collar的任务场景多变，很难采集到足够的数据。而AI就是需要足够的数据。

只要一个行业能够采集到足够多的数据，才能够使用AI将其自动化。AI 理解蓝领的世界，包括和这个世界互动可能需要至少 5 年时间。





预训练是工程问题，后训练是技术问题(但是几年前预训练是技术问题)。

对于后训练，高质量的数据和改进的算法能够极大地提升模型效果。预训练的数据是把整个互联网的数据拿来训练，而后训练数据是结构化的，并且与应用场景高度相关。



垂直模型。并没有真正的垂直模型，垂直模型的通用能力不能差。



模型评估很难但是也很重要。



数据决定模型上限而算法决定模型下限。就目前来说，我们离 AGI 还很远， AGI 能够做自主的学习，我们目前的模型就是填鸭式状态。



自建集群和租云服务器成本差不多，在存储与带宽方面能够省很多。



现在的LLM和过去并没有很大区别，还是吃数据，评估还是很重要，所以很多之前的经验还是能用过来，可以参考借鉴。

LLM的困难在于它的大，而带来很多工程方面的问题，算法探索不够。



# GPGPU视角下的大算力与大模型的供需关系

觉得这篇文章讲的非常好，简单记录一下[GPGPU视角下的大算力与大模型的供需关系](https://www.metax-tech.com/ndetail/12438.html)



大模型的商业模式类似于集成电路产业：

● Google类似于Intel，走IDM (Integrated device manufacturer，整合器件制造商) 路线，预训练和微调都能搞，同时自己的搜索业务也是自研大模型的最大客户。

● OpenAI类似TSMC，主攻预训练，是产业链上游的fab。当下大模型预训练百花齐放，类似2000年后大量出现的fab。五年后的大模型预训练，需要大量工程师进行数据清洗。这将很像TSMC建立新产线，进行良率爬升时，“十万青年十万肝，加班加点救台湾”。于是大模型预训练fab也将会进行横向资源整合。

● 微软、Jasper.ai等公司类似AMD、高通，主攻面向特定应用领域的微调，是产业链中游的fabless。由于微调中有大量know-how技巧，这一环节将是风险最高、利润最高的地方。

● 产业链下游则是需求各异的海量客户。其中有钱的、在意数据隐私的大客户会直接找fabless定制微调后的大模型，类似现在微软找AMD定制数据中心CPU；钱少的、愿意“拿隐私换方便”的小客户，则将通过云端推理的方式直接使用大模型工具。



对于一个LLM，训练分为4个阶段：预训练，监督微调，奖励建模，强化学习



GPT-3的参数量有175B，分为：

1. 线性层： $W_{q},W_{k},W_{v}$以及FFN层，一共$12d_{model}^{2}$
1. FFN层的bias：$5d_{model}$

GPT-3的$d_{model}=12288$，$d_{head}=128$，$h=96$，$d_{ff}=4d_{model}=49152$，$L=96$​，带入式子算起来大概就有174B，再加上输入和输出的token2embedding以及embedding2token，以及positional encoding，就得到了175B。

在scaling law的引导下，参数量会越来越大，伴随而来的是要求更多的训练数据。

对于英文而言，Wikipedia + ArXiv + C4 + Github + Common Crawler的数据量几乎足以满足需求。但中文能否有如此高质量的语料数据库，目前依然存疑。



Why Transformer?

1. 
   推理时表达能力极强。Transformer之前，视觉任务主流模型是卷积神经网络 (CNN)，语言任务主流模型是LSTM。卷积神经网络的痛点是“近视”，因为卷积滑窗感受野有限，难以在图片上相距较远的两个像素间建立联系。LSTM的痛点是“死板”，它在生成下一个token时，会优先考虑紧邻的上一个token，因而会被自然语言中大量无意义的助词、介词干扰。这两个问题都被transformer的attention机制完美解决。
2. 训练时transformer每层都可微分，且梯度平滑。没错，2023年了人类还在用梯度下降法训练神经网络，而且2030年大概依然在用梯度下降法。配合残差网络技术，**算法**层面上大模型可以被有效训练了。
3. 推理和训练过程对于GPGPU非常友好。CNN和LSTM的主要算子对GPU其实都不够友好。比如GPU做卷积是需要用类似im2col的算法转为矩阵乘法，影响计算效率。
4. Transformer中全是矩阵乘法。考虑到GPGPU架构就是为高效执行矩阵乘法设计的。而在2008年之后，GPGPU暴算力的速度远快于其它类型的芯片，最终在2020年左右**算力**终于满足了训练大模型的需求。







预训练成本：以GPT-3为例，在A100平台上的训练成本大概是85万美元。

推理成本：如何降低推理成本是LLM的盈利难点

sft成本：**微调的算力成本远大于预训练，难度也远高于预训练**





**从供给侧来看，摩尔定律面临边际效用递减的困境**

摩尔定律不是一个定律，而是由芯片相关多个表征参数所构成的定律族。这些参数的共同特征都是每两年翻一番或折一半。
