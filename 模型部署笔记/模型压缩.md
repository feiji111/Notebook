# 模型压缩



## 1、网络的构成

权重 + 计算op（算子）

**量化就是将我们训练好的模型，不论是权重、还是计算op，都转换为低精度去计算**

一般来说，模型训练的权重值是FP32。

FP32 --> FP16也是量化，这种方式量化几乎无损，不需要校准

FP32 --> INT8量化，有损，需要校准

## 2、量化过程

量化的两个重要过程：量化与反量化。

<img src="assets/640.webp" alt="Image" style="zoom:50%;" />

量化实际上就是通过一定的映射关系，将FP32等高精度的类型映射到相应的低精度的类型。

**不同的映射关系也意味着不同的量化方案。**



### 2.1 数据类型

在正式讲量化前，有必要了解模型训练与推理时的数据类型。

许多浮点数标准是由IEEE 754规定的。

参考：

1. [FP64, FP32, FP16, BFLOAT16, TF32, and other members of the ZOO | by Grigory Sapunov | Medium](https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407)

### 2.1.1 FP80

<img src="assets/762px-X86_Extended_Floating_Point_Format.svg.png" alt="img" style="zoom:50%;" />

FP80用于对精度有非常高要求的科学计算中，不会在神经网络的训练和推理中使用

### 2.1.2 FP64

FP64用于对精度有要求的科学计算中，不会在神经网络的训练和推理中使用。但是TF与PyTorch框架有相应的支持。

### 2.1.3 FP32

FP32是网络训练与推理时的标准数据类型。权重，激活以及优化器的数据类型都是FP32。

FP32的精度低，不适用于科学计算。

### 2.1.4 FP16

目前来看，在深度学习领域，人们逐渐开始采用FP16+FP32的混合精度训练方式训练。



FP16 1位符号位(Sign)，5位指数位(Exponent)，10位尾数位(Fraction)。



### 2.1.5 BF16

BF16由谷歌提出。BF16目前正在逐渐替代FP16。并且硬件厂商(NVIDA A100，Intel Xeon)也在不断增加对于BF16的支持。



BF16 1位符号位(Sign)，8位指数位(Exponent)，7位尾数位(Fraction)。

相比于FP16，BF16精度更低，但是有更宽的动态范围。BF16的指数部分位宽与FP32相同，因此其动态范围与FP32也相同。这个性质使得其非常适合于模型的训练。FP32能够非常快地转变为BF16，只需要截断一部分尾数位即可。**特别是在LLM中，BF16已经是训练和推理的标准格式。**

但是对于BF16的硬件支持，并不是非常广泛。GPU上NVIDIA Ampere架构(A100)之后的GPU，ASIC方面Google TPU v2/v3，CPU方面Intel Xeon Cooper Lake架构之后的CPU才支持BF16。



### 2.1.6 TF32

NVIDIA A100中新增加的浮点类型。

TF32 1位符号位(Sign)，8位指数位(Exponent)，10位尾数位(Fraction)，一共19位。



### 2.1.7 INT8



### 2.1.8 INT4，INT3，INT2



### 2.1.9 总结

FP16，BF16以及FP32是LLM在训练推理过程常见的三种类型。

采用不同的数据类型不仅仅会影响到显存占用，还会影响到训练和推理的速度。

IEEE 754标准下的浮点数最大的问题是浮点数之间的转换不方便，因此有人也提出了新的浮点数标准，在不同精度的浮点数之间的转换更加方便(**Posit**标准)。





### 2.2 基于线性量化的对称量化(Scale Quantization)和非对称量化(Affine Quantization)

下面的示例来自于NVIDIA的**[INTEGER QUANTIZATION FOR DEEP LEARNING INFERENCE: PRINCIPLES AND EMPIRICAL EVALUATION]**

<img src="assets/640-1712240445887-164.webp" alt="Image" style="zoom:50%;" />

非对称量化和对称量化的不同就在于实数0能否对应着整数0。

对称量化和非对称量化都属于**线性量化**

```
线性量化将FP32映射到INT8数据类型，每个间隔是相等的，而不相等的就称为非线性量化。
```

**对称量化公式**

<img src="assets/640-1712240447608-167.webp" alt="Image" style="zoom:50%;" />

实际上对称量化核心就是以下公式
$$
x_{int} = s * x
$$
clip操作用于当x-int超过范围时将超过范围的int类型映射到量化后范围的最大值/最小值

通过上面的式子，当采用8bit int类型时，量化后的范围是**[-127, 127]**(-128没有被采用，一种说法是为了范围的对称，另一种说法是为了提高效率？)

**非对称量化公式**

<img src="assets/640-1712240449090-170.webp" alt="Image" style="zoom:50%;" />

实际上非对称量化就是以下公式
$$
x_{int} = s * x + z
$$

clip的作用与上面相同。

当采用8bit int类型时，量化后的范围是**[-128, 127]**

其中**α**是输入数据分布中的实数最大值，而**β**是输入数据分布中的实数最小值。

当前**不同的量化算法和优化策略往往是寻找一个恰当的[**α,β**](即最小值和最大值)，使得 clip 和 round 操作导致的误差较小。**

**反量化公式**

<img src="assets/640-1712240450593-173.webp" alt="Image" style="zoom:50%;" />



对于量化的公式，不同论文给出的不太一样，但是基本思想都是一样，总体大差不差。量化更多的是一个工程方面的问题，在实现细节方面可能会存在一些差别。

这是LLM.int8()(bitsandbytes，**bnb**)给出的量化公式

**absolute maximum quantization(对称量化，symmetric quantization)：**

<img src="assets/image-20240923195014824.png" alt="image-20240923195014824" style="zoom:50%;" />



**Zeropoint quantization(非对称量化，asymmetric quantization)**：

<img src="assets/image-20240923195129366.png" alt="image-20240923195129366" style="zoom:50%;" />



而在谷歌的**[Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference]**中，将上面的两种用一个式子统一起来

<img src="assets/image-20240924160215561.png" alt="image-20240924160215561" style="zoom:67%;" />

量化还需要一个量化范围**[a, b]**

<img src="assets/image-20240925190636084.png" alt="image-20240925190636084" style="zoom:50%;" />

而**Z**的计算
$$
z(a, b, n) := (n/2 - 1) - b/s
$$




在谷歌的一篇量化的whitepaper **[Quantizing deep convolutional networks for efficient inference: A whitepaper]**中

<img src="assets/image-20240924163717223.png" alt="image-20240924163717223" style="zoom:50%;" />



<img src="assets/image-20240924163753957.png" alt="image-20240924163753957" style="zoom:50%;" />

<img src="assets/image-20240924163806670.png" alt="image-20240924163806670" style="zoom:50%;" />





回到对称量化与非对称量化，对称量化与非对称量化的区别核心在于Zero-point的选择问题。这个Zero-point是一个整数类型，用于表面浮点数0会被映射到具体哪一个量化后的整数上。

在前向传播的过程中，会存在**padding 0**的操作，而通过引入Zero-point，能够使得浮点数0被量化后不会引入误差(相当于对浮点数值为0的情况单独考虑其量化)。



选择对称量化还是非对称量化是与值的分布有关的。

**一般来说，对于weights，会采用对称量化；对于activation，会采用非对称量化。**

**因为绝大多数weight的分布都是均匀的，基本关于.0对称。而activation则不一样，由于激活函数的存在，大部分情况下的activation分布是不均匀的，只有采用非对称量化数据分布才会和原始的activation分布一致。**



但是非对称量化是有代价的，会使得计算更加复杂。

同时，weights采用对称量化，而activation采用非对称量化还有出于对矩阵乘法计算的复杂度的考虑。



### 2.3 非线性量化

**非线性量化对于部署不太友好，但是量化效果好**。比如**对数量化**，将乘法操作转变为加法操作。



### 2.4 量化粒度

量化粒度有多种：

- **per-tensor**
- **per-channel**
- **per-vector**
- **Global**
- **block**

**pre-tensor**对于同一块输入和激活值，采用一个scale；而**pre-channel**作用于权重，对于每一个通道，采用一个scale。



在模型量化的过程中，有一个**outlier(翻译为离群值)**的概念非常重要。**outlier**是指一些值远大于或者远小于正常范围的参数，这些outlier值会严重影响量化效果。

一方面，一般量化粒度越小(能够避免outlier对量化的影响)，模型精度越高但是相应也会带来更高的计算成本。

另一方面，量化粒度也决定了卷积操作(或者GEMM操作)能否通过INT8实现从而获得相比于FP32卷积的加速。





### 2.5 量化核心操作

量化过程中最核心的操作是**卷积量化**，卷积可以拆分为**im2col+sgemm**。参考这里[卷积加速](#卷积加速)。

卷积表示为矩阵相乘以后
$$
Y=XW
$$
输出Y中的每一个元素是由

<img src="assets/640-1712240454131-176.webp" alt="Image" style="zoom:67%;" />

得到的。

但是每一个元素都有自己的scale，因此需要将这两项给提出来。

<img src="assets/640-1712240456027-179.webp" alt="Image" style="zoom:50%;" />

如果想要将这两项给提出来，那么**输入(activation)每一行必须共享scale而权重每一列需要共享scale**。这就是为什么最小的量化粒度是**per-row**和**per-col**。

此时就是INT8类型的卷积运算，然后再反量化。

硬件对于INT8类型的操作往往有相应的指令集优化，并且INT8类型所需要的内存是FP32的四分之一，可以提高算子的吞吐。相当于减少延迟，提高吞吐。

上面是对于对称量化，对于非对称量化，情况会有一些复杂，计算量更大。


$$
\begin{align}
y_{ij} = \sum_{k = 1}^{p}x_{ik} \cdot w_{kj} \approx \sum_{k=1}^{p}{\rm dequantize}(x_{q,ik},s_{q,ik}) \cdot {\rm dequantize}(w_{q,kj},s_{w,kj}) &= \sum_{k=1}^{p} \frac{1}{s_{x,ik}}(x_{q,ik} - z_{x,i}) \cdot \frac{1}{s_{w,kj}}(x_{q,kj} - z_{w,j}) \\
&= \frac{1}{s_{x,ik}}\frac{1}{s_{w,kj}}
\end{align}
$$




<img src="assets/image-20241010193804249.png" alt="image-20241010193804249" style="zoom: 67%;" />









回到反量化的问题上来，最后INT8类型的矩阵乘法前的
$$
\frac{1}{s_{x,i} \cdot s_{w,j}}
$$
这一步操作，就是在做反量化。



在NVIDIA的量化报告**INTEGER QUANTIZATION FOR DEEP LEARNING INFERENCE: PRINCIPLES AND EMPIRICAL EVALUATION**中，将量化分为两类**uniform**与**non-uniform**。

**uniform的量化就是允许算数量化的一类量化，而non-uniform的量化就是不允许量化算数，在计算之前现需要反量化回原始精度在做计算的一类量化。**



### 2.6 动态范围选择(Calibration)

为了计算`scale`以及非对称量化中的`z`，需要知道`FP32 weight/activation` 的实际动态范围(找到**α** and **β**)。而`activation`的动态范围是会变化的，所以需要通过`calibration`校准，从而采样出实际的动态范围，这也是为什么一些PTQ方法需要一个校准数据集。



但校准集并不能代表所有的输入情况，



量化的误差来自于两个方面，clip与round。

动态范围的选择实际上就是**clipping error**与**rounding error**的取舍：

1. 在动态范围之外的outlier被clip，这带来了clipping error。
2. 动态范围内浮点数，被round到了最近的整数，这带来了round error。
3. 动态范围如果更小，那么clipping error会更大；但是由于范围小，动态范围内的分辨率更低，导致round error会更小







动态范围的选择算法：

- **MinMax**
- **MovingAverageMinMax**
- **Kullback-Leibler divergence**







**饱和状态与不饱和状态**：

- **饱和状态**：出现某些 `FP32` 数值不在这个实际动态范围之内我们称之为饱和状态
- **不饱和状态**：量化过程中的每一个 `FP32` 数值都在这个实际动态范围内







### 2.7 量化与反量化

**量化技术的早期，只有卷积算子支持量化**，但实际网络中还包含其他算子，而其他算子又只支持 `FP32` 计算，因此需要把 INT8 转换成 FP32，所以需要引入反量化层。

**因此量化还需要考虑算子的支持。**

**量化不仅需要考虑算子的支持，还需要考虑底层硬件的支持**，比如对于NVIDIA的GPU，其就有FP32 Tensor Core与FP16 Tensor Core，分别负责两种精度的矩阵的运算。 

<img src="assets/two-modes-of-operation-on-ampere-tensor-cores.png" alt="FP16/BF16 mode on Ampere provides 2x the throughput compared to TF32." style="zoom: 80%;" />



**为什么不能全网络量化？**

之前纠结过为什么不能够采用一个全量化的方案，只在最后的输出上做反量化，后来明白这样的做法有几个问题：

1. **量化是通过INT8的矩阵乘法(卷积)替换浮点(卷积)，但是还是希望最后的结果是相同的，量化后只是一个等价的实现，如果不反量化，最后得到的结果并不是等价的，整数的结果与浮点数的结果必然误差十分大**
2. **其次，如果不反量化，而是采用INT8的值一路算到底，每一层的INT8输出和浮点实现的输出都会有误差，这个误差会逐层累计**
3. **对于一些非线性操作，比如softmax等，如果采用精度低的INT8，量化带来的精度损失是非常巨大的**
4. **某些算子可能不支持INT8类型，这一点与(3)类似**
5. ......后续可能还有补充或修改



、

## 2.8 量化一个模型方式

获得一个模型的对应量化模型有几种方式：

- **直接量化**
- **量化 + calibrator **
- **基于finetune**



PTQ



QAT(Quantization-aware training，也叫伪量化，在线量化)

<img src="assets/image-20240924160340233.png" alt="image-20240924160340233" style="zoom:50%;" />

### 2.9 其它一些需要注意的

模型做推理时，权重只占很少一部分的内存，大部分内存占用来自激活值activation。如果做低比特量化只关注卷积的话（很多论文其实也是只量化了卷积），那么是无法带来内存占用降低的。

因此需要将更多的activation进行量化，但是这样做会带来更多的精度损失。



## 2.10 另一种量化

上面讲到的一系列量化方式，都属于RTN(round-to-nearest)范畴的量化。另一种量化的方式是把量化问题看作是优化问题，通过最小化量化损失来获得最优的量化方案，但本质上最终还是需要得到量化的scale和zero point。

这一方面的工作有：

1. ZeroQuant
2. GPTQ



GPTQ采用的思想就是找到一个量化后的权重能够最小化下面的loss
$$
argmin_{\mathbf{\widehat{W}}} = {\lVert \mathbf{WX - \widehat{W}X} \rVert}^{2}_{2}
$$




# 目前的一些量化方案

- Pytorch
- TensorRT
- NNCF
- AIMET
- TensorRT的Pytorch-quantization，主要针对Pytorch（TF已经有很好用的官方量化工具了）
- **TVM**
- OpenVINO
- NCNN



具体方法有：

- `data free`：不使用校准集，传统的方法直接将浮点参数转化成量化数，使用上非常简单，但是一般会带来很大的精度损失，但是高通最新的论文 `DFQ` 不使用校准集也得到了很高的精度。
- `calibration`：基于校准集方案，通过输入少量真实数据进行统计分析。很多芯片厂商都提供这样的功能，如 `tensorRT`、高通、海思、地平线、寒武纪
- `finetune`：基于训练 `finetune` 的方案，将量化误差在训练时仿真建模，调整权重使其更适合量化。好处是能带来更大的精度提升，缺点是要修改模型训练代码，开发周期较长。



另一些量化方案：

1. **二值化**，其可以用简单的位运算来同时计算大量的数。对比从 nvidia gpu 到 x86 平台，1bit 计算分别有 5 到128倍的理论性能提升。且其只会引入一个额外的量化操作，该操作可以享受到 SIMD（单指令多数据流）的加速收益。
2. **线性量化**(最常见)，又可细分为非对称，对称和 `ristretto` 几种。在 `nvdia gpu`，`x86`、`arm` 和 部分 `AI` 芯片平台上，均支持 `8bit` 的计算，效率提升从 `1` 倍到 `16` 倍不等，其中 `tensor core` 甚至支持 `4bit`计算，这也是非常有潜力的方向。**线性量化引入的额外量化/反量化计算都是标准的向量操作**，因此也可以使用 `SIMD` 进行加速，带来的额外计算耗时不大。
3. **对数量化**，一种比较特殊的量化方法。两个同底的幂指数进行相乘，那么等价于其指数相加，降低了计算强度。同时加法也被转变为索引计算。目前 `nvdia gpu`，`x86`、`arm` 三大平台上没有实现对数量化的加速库，但是目前已知海思 `351X` 系列芯片上使用了对数量化。



工业界和学术界的量化不太一样。

从**[Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference]**这篇谷歌提出的量化方案来看，这篇文章中采用的是全量化网络，对输入做量化然后输出做反量化。中间的计算都是定点计算。

这篇文章的量化方案在TFLite中有对应的实现，参考[LiteRT 8-bit quantization specification  | Google AI Edge  | Google AI for Developers](https://ai.google.dev/edge/litert/models/quantization_spec)。TensorFlow/TFLite非常早就支持了PTQ与QAT。

虽然谷歌上面的那篇文章只对输出做了反量化，对于网络中间的量不做反量化。但是其网络中也有类似的结构，与反量化的作用类似，只不过执行的操作是定点操作。







TensorRT的量化方案与TF类似但也有不同。

TensorRT的量化方式有两种`implicitly`以及`explicitly`量化。





在线量化与离线量化



# 卷积加速

## 1、FFT

通过快速傅里叶变换，将卷积转变为矩阵乘积。

但是由于feature map和卷积核的尺寸不同，因此需要补0。

## 2、im2col+sgemm

同样是将卷积转变为矩阵乘法。

这种方法的核心思想是不论是图像还是卷积核，都是按照**行优先**存储的。

### 2.1 im2col

**im2col**就是将输入图像的像素点按照“卷积区域”，挨个排列起来。

<img src="assets/640-1712240462776-182.webp" alt="图片" style="zoom:50%;" />

<img src="assets/640-1712240465827-185.webp" alt="图片" style="zoom:50%;" />

对于多通道的输入图像

![图片](assets/640-1712240468845-188.webp)

### 2.2 Sgemm（Single precision GEneral Matrix Multiply）

Sgemm是单精度矩阵乘法。除了Sgemm,还有Dgemm，Cgemm，Zgemm，Dsymm，Zhemm等。

## 3、Winograd

出自于[Fast Algorithms for Convolutional Neural Networks](https://arxiv.org/abs/1509.09308)

## 4. Fourier



# 矩阵乘法的加速

最原始的矩阵乘法的复杂度是O(n^3)，n是矩阵的维度（假设是方阵）



**Strassen's Algorithm**



**Winograd**

证明了



**AlphaTensor**



# BLAS

**Basic Linear Algebra Subprograms** (**BLAS**)是一个针对线性代数库的通用标准与规范，规定了一系列底层用于线性代数计算的函数的接口(与OpenGL相同)。



BLAS最早是由Fortran编写，作为一个Fortran库。Fortran版本的BLAS是作为一个reference implementation，优化并不是特别好。后续也有C语言版本的cBLAS。

有许多BLAS的实现，基本都遵循BLAS的接口规范,，而且一部分是专门为某一些硬件优化的，利用到了特定的硬件特性：

- [cuBLAS](https://en.wikipedia.org/wiki/CUDA#Programming_abilities)
- [rocBLAS](https://en.wikipedia.org/wiki/ROCm#rocBLAS_/_hipBLAS)
- [OpenBLAS](https://en.wikipedia.org/wiki/OpenBLAS)
- [BLIS (BLAS-like Library Instantiation Software)](https://en.wikipedia.org/wiki/BLIS_(software))
- [Intel Math Kernel Library](https://en.wikipedia.org/wiki/Intel_Math_Kernel_Library) (iMKL)
- [ATLAS](https://en.wikipedia.org/wiki/Automatically_Tuned_Linear_Algebra_Software)

许多数值计算的软件或者包都利用到了BLAS-compatible库来进行线性代数计算，比较知名的有MATLAB，NumPy，R，LAPACK，LINPACK，GNU Octave等。



## GEMM(General Matrix Multiply)





# 稀疏化

稀疏在不同的语境下有不同的含义：

- 对于模型压缩领域，稀疏化是压缩模型的一种方法
- 对于大模型领域，稀疏模型指模型具有非常大的容量，但只有模型的用于给定的任务、样本或标记的某些部分被激活。与之相关的还有**MoE**以及**密集模型**。



稀疏方式分类

- **结构化稀疏**  结构化稀疏能够获得规则的稀疏化结构，但是无法获得高的压缩率。精度低。`removes parameters in groups`
- **非结构化稀疏** 非结构化稀疏能够获得更高的压缩率，但是无法保持一个规则的稀疏化结构，实际应用中很难做推理加速。但是精度高。

按照稀疏的粒度来划分，有可以分为：

- **细粒度稀疏** 细粒度的稀疏在单个权重的维度上进行稀疏 
- **粗粒度稀疏** 粗粒度的稀疏在filter/channel维度上进行稀疏



<img src="assets/image-20240613130929007.png" alt="image-20240613130929007" style="zoom: 67%;" />

a、b、c分别为非结构化稀疏、粗粒度结构化稀疏、细粒度结构化稀疏。



## 相关论文



# 训练加速

## Transformer的训练加速

分为token reduction和parameter reduction：

- token reduction使用更少数量的token但是参数量不变
- parameter reduction使用更少的参数
- data reduction则是在数据层面减少冗余



## CNN的训练加速

- 剪枝梯度
- 量化





# Network Architecture Search
