# 强化学习

首先是一些概念：

1. policy
2. Bellman Equation与Bellman Optimal Equation。Bellman Optimal Equation就是最优策略下的Bellman Equation。最佳化方法能够达到最佳化的必要条件。

强化学习可以分为value-based和policy-based/model-based和model-free/online和offline/on-policy和off-policy





1. 强化学习由一个agent和environment组成，agent的行为由一个策略policy决定
2. agent的目标是最大化收益
3. 首先有一个策略
4. 11111111111111111111111111111111111110444444444444444444444444444444444444444444444444444444444444444444444444442122222212
5. 如何定义最优策略？
6. 引出评估策略的两个函数：策略的状态-价值函数和策略的动作-价值函数。这两个函数可以相互表示。
7. 最优策略下的**状态-价值函数**和最优策略下的**动作-价值函数**
8. 贝尔曼方程简化，在强化学习中贝尔曼方程告诉我们价值函数可以通过**迭代**求得



# Bellman Equation

贝尔曼方程中蕴含着一个优化问题



## RLHF



## A2C



## A3C



## PPO(Proximal Policy Optimization)

PPO涉及到4个模型。

## DPO



## SFT与RL







## RL框架

RL同样也有框架，比如LLamaFactory、Deepspeed-chat和OpenRLHF等。

1. DeepSpeed-Chat
2. NeMo-Aligner
3. LLamaFactory
4. OpenRLHF
5. veRL
